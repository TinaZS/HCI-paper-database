[
    {
        "title": "Polymind: Parallel Visual Diagramming with Large Language Models to\n  Support Prewriting Through Microtasks",
        "authors": [
            "Qian Wan",
            "Jiannan Li",
            "Huanchen Wang",
            "Zhicong Lu"
        ],
        "abstract": "Prewriting is the process of generating and organising ideas before a first\ndraft. It consists of a combination of informal, iterative, and semi-structured\nstrategies such as visual diagramming, which poses a challenge for\ncollaborating with large language models (LLMs) in a turn-taking conversational\nmanner. We present Polymind, a visual diagramming tool that leverages multiple\nLLM-powered agents to support prewriting. The system features a parallel\ncollaboration workflow in place of the turn-taking conversational interactions.\nIt defines multiple ``microtasks'' to simulate group collaboration scenarios\nsuch as collaborative writing and group brainstorming. Instead of repetitively\nprompting a chatbot for various purposes, Polymind enables users to orchestrate\nmultiple microtasks simultaneously. Users can configure and delegate customised\nmicrotasks, and manage their microtasks by specifying task requirements and\ntoggling visibility and initiative. Our evaluation revealed that, compared to\nChatGPT, users had more customizability over collaboration with Polymind, and\nwere thus able to quickly expand personalised writing ideas during prewriting.",
        "link": "http://arxiv.org/abs/2502.09577v1",
        "published_date": "2025-02-13T18:34:52Z",
        "doi": null
    },
    {
        "title": "Mind the Gap! Choice Independence in Using Multilingual LLMs for\n  Persuasive Co-Writing Tasks in Different Languages",
        "authors": [
            "Shreyan Biswas",
            "Alexander Erlei",
            "Ujwal Gadiraju"
        ],
        "abstract": "Recent advances in generative AI have precipitated a proliferation of novel\nwriting assistants. These systems typically rely on multilingual large language\nmodels (LLMs), providing globalized workers the ability to revise or create\ndiverse forms of content in different languages. However, there is substantial\nevidence indicating that the performance of multilingual LLMs varies between\nlanguages. Users who employ writing assistance for multiple languages are\ntherefore susceptible to disparate output quality. Importantly, recent research\nhas shown that people tend to generalize algorithmic errors across independent\ntasks, violating the behavioral axiom of choice independence. In this paper, we\nanalyze whether user utilization of novel writing assistants in a charity\nadvertisement writing task is affected by the AI's performance in a second\nlanguage. Furthermore, we quantify the extent to which these patterns translate\ninto the persuasiveness of generated charity advertisements, as well as the\nrole of peoples' beliefs about LLM utilization in their donation choices. Our\nresults provide evidence that writers who engage with an LLM-based writing\nassistant violate choice independence, as prior exposure to a Spanish LLM\nreduces subsequent utilization of an English LLM. While these patterns do not\naffect the aggregate persuasiveness of the generated advertisements, people's\nbeliefs about the source of an advertisement (human versus AI) do. In\nparticular, Spanish-speaking female participants who believed that they read an\nAI-generated advertisement strongly adjusted their donation behavior downwards.\nFurthermore, people are generally not able to adequately differentiate between\nhuman-generated and LLM-generated ads. Our work has important implications for\nthe design, development, integration, and adoption of multilingual LLMs as\nassistive agents -- particularly in writing tasks.",
        "link": "http://arxiv.org/abs/2502.09532v1",
        "published_date": "2025-02-13T17:49:30Z",
        "doi": null
    },
    {
        "title": "Computational techniques enabling the perception of virtual images\n  exclusive to the retinal afterimage",
        "authors": [
            "Staas de Jong",
            "Gerrit van der Veer"
        ],
        "abstract": "The retinal afterimage is a widely known effect in the human visual system,\nwhich has been studied and used in the context of a number of major art\nmovements. Therefore, when considering the general role of computation in the\nvisual arts, this begs the question whether this effect, too, may be induced\nusing partly automated techniques. If so, it may become a computationally\ncontrollable ingredient of (interactive) visual art, and thus take its place\namong the many other aspects of visual perception which already have preceded\nit in this sense. The present moment provides additional inspiration to lay the\ngroundwork for extending computer graphics in general with the retinal\nafterimage: Historically, we are in a phase where some head-mounted\nstereoscopic AR/VR technologies are now providing eye tracking by default,\nthereby allowing realtime monitoring of the processes of visual fixation that\ncan induce the retinal afterimage. A logical starting point for general\ninvestigation is then shape display via the retinal afterimage, since shape\nrecognition lends itself well to unambiguous reporting. Shape recognition,\nhowever, may also occur due to normal vision, which happens simultaneously.\nCarefully and rigorously excluding this possibility, we develop computational\ntechniques enabling shape display exclusive to the retinal afterimage.",
        "link": "http://arxiv.org/abs/2502.09435v1",
        "published_date": "2025-02-13T15:59:57Z",
        "doi": "10.3390/bdcc6030097"
    },
    {
        "title": "Code Style Sheets: CSS for Code",
        "authors": [
            "Sam Cohen",
            "Ravi Chugh"
        ],
        "abstract": "Program text is rendered using impoverished typographic styles. Beyond choice\nof fonts and syntax-highlighting colors, code editors and related tools utilize\nvery few text decorations. These limited styles are, furthermore, applied in\nmonolithic fashion, regardless of the programs and tasks at hand.\n  We present the notion of code style sheets for styling the textual\nrepresentation of programs. Motivated by analogy to cascading style sheets\n(CSS) for styling HTML documents, code style sheets provide mechanisms for\ndefining rules to select and style abstract syntax trees (ASTs). Technically,\ncode style sheets generalize notions from CSS over untyped HTML trees to a\nprogramming-language setting with algebraic data types (e.g. ASTs).\nPractically, code style sheets allow ASTs to be styled granularly, based on\nsemantic information -- such as the structure of abstract syntax, static type\ninformation, and corresponding run-time values -- as well as design choices on\nthe part of authors and readers of a program. In this paper, we design and\nimplement a code style sheets system for a subset of Haskell -- the rich\nsyntactic and semantic structure of Haskell provide a fertile first setting in\nwhich to explore the notion of code style sheets. We illustrate several use\ncases involving code presentation and visualization tasks.",
        "link": "http://arxiv.org/abs/2502.09386v1",
        "published_date": "2025-02-13T15:02:38Z",
        "doi": null
    },
    {
        "title": "Let's Talk Futures: A Literature Review of HCI's Future-Orientation",
        "authors": [
            "Camilo Sanchez",
            "Sui Wang",
            "Kaisa Savolainen",
            "Felix Anand Epp",
            "Antti Salovaara"
        ],
        "abstract": "HCI is future-oriented by nature: it explores new human--technology\ninteractions and applies the findings to promote and shape vital visions of\nsociety. Still, the visions of futures in HCI publications seem largely\nimplicit, techno-deterministic, narrow, and lacking in roadmaps and attention\nto uncertainties. A literature review centered on this problem examined\nfuturing and its forms in the ACM Digital Library's most frequently cited HCI\npublications. This analysis entailed developing the four-category framework\nSPIN, informed by futures studies literature. The results confirm that, while\ntechnology indeed drives futuring in HCI, a growing body of HCI research is\ncoming to challenge techno-centric visions. Emerging foci of HCI futuring\ndemonstrate active exploration of uncertainty, a focus on human experience, and\ncontestation of dominant narratives. The paper concludes with insight\nilluminating factors behind techno-centrism's continued dominance of HCI\ndiscourse, as grounding for five opportunities for the field to expand its\ncontribution to futures and anticipation research.",
        "link": "http://arxiv.org/abs/2502.09362v1",
        "published_date": "2025-02-13T14:31:25Z",
        "doi": "10.1145/3706598.3713759"
    },
    {
        "title": "FE-LWS: Refined Image-Text Representations via Decoder Stacking and\n  Fused Encodings for Remote Sensing Image Captioning",
        "authors": [
            "Swadhin Das",
            "Raksha Sharma"
        ],
        "abstract": "Remote sensing image captioning aims to generate descriptive text from remote\nsensing images, typically employing an encoder-decoder framework. In this\nsetup, a convolutional neural network (CNN) extracts feature representations\nfrom the input image, which then guide the decoder in a sequence-to-sequence\ncaption generation process. Although much research has focused on refining the\ndecoder, the quality of image representations from the encoder remains crucial\nfor accurate captioning. This paper introduces a novel approach that integrates\nfeatures from two distinct CNN based encoders, capturing complementary\ninformation to enhance caption generation. Additionally, we propose a weighted\naveraging technique to combine the outputs of all GRUs in the stacked decoder.\nFurthermore, a comparison-based beam search strategy is incorporated to refine\ncaption selection. The results demonstrate that our fusion-based approach,\nalong with the enhanced stacked decoder, significantly outperforms both the\ntransformer-based state-of-the-art model and other LSTM-based baselines.",
        "link": "http://arxiv.org/abs/2502.09282v1",
        "published_date": "2025-02-13T12:54:13Z",
        "doi": null
    },
    {
        "title": "ASP-driven User-interaction with Clinguin",
        "authors": [
            "Alexander Beiser",
            "Susana Hahn",
            "Torsten Schaub"
        ],
        "abstract": "We present clinguin, a system for ASP-driven user interface design. Clinguin\nstreamlines the development of user interfaces for ASP developers by letting\nthem build interactive prototypes directly in ASP, eliminating the need for\nseparate frontend languages. To this end, clinguin uses a few dedicated\npredicates to define user interfaces and the treatment of user-triggered\nevents. This simple design greatly facilitates the specification of user\ninteractions with an ASP system, in our case clingo.",
        "link": "http://arxiv.org/abs/2502.09222v1",
        "published_date": "2025-02-13T11:50:51Z",
        "doi": "10.4204/EPTCS.416.19"
    },
    {
        "title": "Revisiting Euclidean Alignment for Transfer Learning in EEG-Based\n  Brain-Computer Interfaces",
        "authors": [
            "Dongrui Wu"
        ],
        "abstract": "Due to the non-stationarity and large individual differences of EEG signals,\nEEG-based brain-computer interfaces (BCIs) usually need subject-specific\ncalibration to tailor the decoding algorithm for each new subject, which is\ntime-consuming and user-unfriendly, hindering their real-world applications.\nTransfer learning (TL) has been extensively used to expedite the calibration,\nby making use of EEG data from other subjects/sessions. An important\nconsideration in TL for EEG-based BCIs is to reduce the data distribution\ndiscrepancies among different subjects/session, to avoid negative transfer.\nEuclidean alignment (EA) was proposed in 2020 to address this challenge.\nNumerous experiments from 10 different BCI paradigms demonstrated its\neffectiveness and efficiency. This paper revisits the EA, explaining its\nprocedure and correct usage, introducing its applications and extensions, and\npointing out potential new research directions. It should be very helpful to\nBCI researchers, especially those who are working on EEG signal decoding.",
        "link": "http://arxiv.org/abs/2502.09203v1",
        "published_date": "2025-02-13T11:43:43Z",
        "doi": null
    },
    {
        "title": "LLM-Driven Augmented Reality Puppeteer: Controller-Free Voice-Commanded\n  Robot Teleoperation",
        "authors": [
            "Yuchong Zhang",
            "Bastian Orthmann",
            "Michael C. Welle",
            "Jonne Van Haastregt",
            "Danica Kragic"
        ],
        "abstract": "The integration of robotics and augmented reality (AR) presents\ntransformative opportunities for advancing human-robot interaction (HRI) by\nimproving usability, intuitiveness, and accessibility. This work introduces a\ncontroller-free, LLM-driven voice-commanded AR puppeteering system, enabling\nusers to teleoperate a robot by manipulating its virtual counterpart in real\ntime. By leveraging natural language processing (NLP) and AR technologies, our\nsystem -- prototyped using Meta Quest 3 -- eliminates the need for physical\ncontrollers, enhancing ease of use while minimizing potential safety risks\nassociated with direct robot operation. A preliminary user demonstration\nsuccessfully validated the system's functionality, demonstrating its potential\nfor safer, more intuitive, and immersive robotic control.",
        "link": "http://arxiv.org/abs/2502.09142v1",
        "published_date": "2025-02-13T10:17:12Z",
        "doi": null
    },
    {
        "title": "Bridging the Gap Between LLMs and Human Intentions: Progresses and\n  Challenges in Instruction Understanding, Intention Reasoning, and Reliable\n  Generation",
        "authors": [
            "Zongyu Chang",
            "Feihong Lu",
            "Ziqin Zhu",
            "Qian Li",
            "Cheng Ji",
            "Zhuo Chen",
            "Yang Liu",
            "Ruifeng Xu",
            "Yangqiu Song",
            "Shangguang Wang",
            "Jianxin Li"
        ],
        "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in\nunderstanding and generation. However, when interacting with human instructions\nin real-world scenarios, LLMs still face significant challenges, particularly\nin accurately capturing and comprehending human instructions and intentions.\nThis paper focuses on three challenges in LLM-based text generation tasks:\ninstruction understanding, intention reasoning, and reliable generation.\nRegarding human complex instruction, LLMs have deficiencies in understanding\nlong contexts and instructions in multi-round conversations. For intention\nreasoning, LLMs may have inconsistent command reasoning, difficulty reasoning\nabout commands containing incorrect information, difficulty understanding user\nambiguous language commands, and a weak understanding of user intention in\ncommands. Besides, In terms of reliable generation, LLMs may have unstable\ngenerated content and unethical generation. To this end, we classify and\nanalyze the performance of LLMs in challenging scenarios and conduct a\ncomprehensive evaluation of existing solutions. Furthermore, we introduce\nbenchmarks and categorize them based on the aforementioned three core\nchallenges. Finally, we explore potential directions for future research to\nenhance the reliability and adaptability of LLMs in real-world applications.",
        "link": "http://arxiv.org/abs/2502.09101v1",
        "published_date": "2025-02-13T09:19:42Z",
        "doi": null
    }
]