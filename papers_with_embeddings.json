[
    {
        "title": "SportsBuddy: Designing and Evaluating an AI-Powered Sports Video\n  Storytelling Tool Through Real-World Deployment",
        "authors": [
            "Tica Lin",
            "Ruxun Xiang",
            "Gardenia Liu",
            "Divyanshu Tiwari",
            "Meng-Chia Chiang",
            "Chenjiayi Ye",
            "Hanspeter Pfister",
            "Chen Zhu-Tien"
        ],
        "abstract": "Video storytelling is essential for sports performance analysis and fan\nengagement, enabling sports professionals and fans to effectively communicate\nand interpret the spatial and temporal dynamics of gameplay. Traditional\nmethods rely on manual annotation and verbal explanations, placing significant\ndemands on creators for video editing skills and on viewers for cognitive\nfocus. However, these approaches are time-consuming and often struggle to\naccommodate individual needs. SportsBuddy addresses this gap with an intuitive,\ninteractive video authoring tool. It combines player tracking, embedded\ninteraction design, and timeline visualizations to seamlessly integrate\nnarratives and visual cues within game contexts. This empowers users to\neffortlessly create context-driven video stories. Since its launch, over 150\nsports users, including coaches, athletes, content creators, parents and fans,\nhave utilized SportsBuddy to produce compelling game highlights for diverse use\ncases. User feedback highlights its accessibility and ease of use, making video\nstorytelling and insight communication more attainable for diverse audiences.\nCase studies with collegiate teams and sports creators further demonstrate\nSportsBuddy's impact on enhancing coaching communication, game analysis, and\nfan engagement.",
        "link": "http://arxiv.org/abs/2502.08621v1",
        "published_date": "2025-02-12T18:16:11Z",
        "doi": null
    },
    {
        "title": "AR Glulam: Accurate Augmented Reality Using Multiple Fiducial Markers\n  for Glulam Fabrication",
        "authors": [
            "Alexander Htet Kyaw",
            "Arvin Xu",
            "Sasa Zivkovic",
            "Gwyllim Jahn",
            "Cameron Newnham",
            "Nick Van Den Berg"
        ],
        "abstract": "Recent advancements in Augmented Reality (AR) have demonstrated applications\nin architecture, design, and fabrication. Compared to conventional 2D\nconstruction drawings, AR can be used to superimpose contextual instructions,\ndisplay 3D spatial information and enable on-site engagement. Despite the\npotential of AR, the widespread adoption of the technology in the industry is\nlimited by its precision. Precision is important for projects requiring strict\nconstruction tolerances, design fidelity, and fabrication feedback. For\nexample, the manufacturing of glulam beams requires tolerances of less than\n2mm. The goal of this project is to explore the industrial application of using\nmultiple fiducial markers for high-precision AR fabrication. While the method\nhas been validated in lab settings with a precision of 0.97, this paper focuses\non fabricating glulam beams in a factory setting with an industry manufacturer,\nUnalam Factory.",
        "link": "http://arxiv.org/abs/2502.08566v1",
        "published_date": "2025-02-12T16:56:07Z",
        "doi": null
    },
    {
        "title": "Fostering Appropriate Reliance on Large Language Models: The Role of\n  Explanations, Sources, and Inconsistencies",
        "authors": [
            "Sunnie S. Y. Kim",
            "Jennifer Wortman Vaughan",
            "Q. Vera Liao",
            "Tania Lombrozo",
            "Olga Russakovsky"
        ],
        "abstract": "Large language models (LLMs) can produce erroneous responses that sound\nfluent and convincing, raising the risk that users will rely on these responses\nas if they were correct. Mitigating such overreliance is a key challenge.\nThrough a think-aloud study in which participants use an LLM-infused\napplication to answer objective questions, we identify several features of LLM\nresponses that shape users' reliance: explanations (supporting details for\nanswers), inconsistencies in explanations, and sources. Through a large-scale,\npre-registered, controlled experiment (N=308), we isolate and study the effects\nof these features on users' reliance, accuracy, and other measures. We find\nthat the presence of explanations increases reliance on both correct and\nincorrect responses. However, we observe less reliance on incorrect responses\nwhen sources are provided or when explanations exhibit inconsistencies. We\ndiscuss the implications of these findings for fostering appropriate reliance\non LLMs.",
        "link": "http://arxiv.org/abs/2502.08554v1",
        "published_date": "2025-02-12T16:35:41Z",
        "doi": "10.1145/3706598.3714020"
    },
    {
        "title": "\"You'll Be Alice Adventuring in Wonderland!\" Processes, Challenges, and\n  Opportunities of Creating Animated Virtual Reality Stories",
        "authors": [
            "Lin-Ping Yuan",
            "Feilin Han",
            "Liwenhan Xie",
            "Junjie Zhang",
            "Jian Zhao",
            "Huamin Qu"
        ],
        "abstract": "Animated virtual reality (VR) stories, combining the presence of VR and the\nartistry of computer animation, offer a compelling way to deliver messages and\nevoke emotions. Motivated by the growing demand for immersive narrative\nexperiences, more creators are creating animated VR stories. However, a\nholistic understanding of their creation processes and challenges involved in\ncrafting these stories is still limited. Based on semi-structured interviews\nwith 21 animated VR story creators, we identify ten common stages in their\nend-to-end creation processes, ranging from idea generation to evaluation,\nwhich form diverse workflows that are story-driven or visual-driven.\nAdditionally, we highlight nine unique issues that arise during the creation\nprocess, such as a lack of reference material for multi-element plots, the\nabsence of specific functionalities for story integration, and inadequate\nsupport for audience evaluation. We compare the creation of animated VR stories\nto general XR applications and distill several future research opportunities.",
        "link": "http://arxiv.org/abs/2502.08513v1",
        "published_date": "2025-02-12T15:46:47Z",
        "doi": "10.1145/3706598.3714257"
    },
    {
        "title": "Computed fingertip touch for the instrumental control of musical sound\n  with an excursion on the computed retinal afterimage",
        "authors": [
            "Staas de Jong"
        ],
        "abstract": "In this thesis, we present an articulated, empirical view on what human music\nmaking is, and on how this fundamentally relates to computation. The\nexperimental evidence which we obtained seems to indicate that this view can be\nused as a tool, to systematically generate models, hypotheses and new\ntechnologies that enable an ever more complete answer to the fundamental\nquestion as to what forms of instrumental control of musical sound are possible\nto implement. This also entails the development of two novel transducer\ntechnologies for computed fingertip touch: The cyclotactor (CT) system, which\nprovides fingerpad-orthogonal force output while tracking surface-orthogonal\nfingertip movement; and the kinetic surface friction transducer (KSFT) system,\nwhich provides fingerpad-parallel force output while tracking surface-parallel\nfingertip movement.\n  In addition to the main research, the thesis also contains two research\nexcursions, which are due to the nature of the Ph.D. position. The first\nexcursion shows how repeated and varying pressing movements on the already\nheld-down key of a computer keyboard can be used both to simplify existing user\ninteractions and to implement new ones, that allow the rapid yet detailed\nnavigation of multiple possible interaction outcomes. The second excursion\nshows that automated computational techniques can display shape specifically in\nthe retinal afterimage, a well-known effect in the human visual system.",
        "link": "http://arxiv.org/abs/2502.08471v1",
        "published_date": "2025-02-12T15:04:42Z",
        "doi": null
    },
    {
        "title": "Blending the Worlds: World-Fixed Visual Appearances in Automotive\n  Augmented Reality",
        "authors": [
            "Robin Connor Schramm",
            "Markus Sasalovici",
            "Jann Philipp Freiwald",
            "Michael Otto",
            "Melissa Reinelt",
            "Ulrich Schwanecke"
        ],
        "abstract": "With the transition to fully autonomous vehicles, non-driving related tasks\n(NDRTs) become increasingly important, allowing passengers to use their driving\ntime more efficiently. In-car Augmented Reality (AR) gives the possibility to\nengage in NDRTs while also allowing passengers to engage with their\nsurroundings, for example, by displaying world-fixed points of interest (POIs).\nThis can lead to new discoveries, provide information about the environment,\nand improve locational awareness. To explore the optimal visualization of POIs\nusing in-car AR, we conducted a field study (N = 38) examining six parameters:\npositioning, scaling, rotation, render distance, information density, and\nappearance. We also asked for intention of use, preferred seat positions and\npreferred automation level for the AR function in a post-study questionnaire.\nOur findings reveal user preferences and general acceptance of the AR\nfunctionality. Based on these results, we derived UX-guidelines for the visual\nappearance and behavior of location-based POIs in in-car AR.",
        "link": "http://arxiv.org/abs/2502.08442v1",
        "published_date": "2025-02-12T14:32:25Z",
        "doi": "10.1145/3706598.3713185"
    },
    {
        "title": "Augmented Journeys: Interactive Points of Interest for In-Car Augmented\n  Reality",
        "authors": [
            "Robin Connor Schramm",
            "Ginevra Fedrizzi",
            "Markus Sasalovici",
            "Jann Philipp Freiwald",
            "Ulrich Schwanecke"
        ],
        "abstract": "As passengers spend more time in vehicles, the demand for non-driving related\ntasks (NDRTs) increases. In-car Augmented Reality (AR) has the potential to\nenhance passenger experiences by enabling interaction with the environment\nthrough NDRTs using world-fixed Points of Interest (POIs). However, the\neffectiveness of existing interaction techniques and visualization methods for\nin-car AR remains unclear. Based on a survey (N=110) and a pre-study (N=10), we\ndeveloped an interactive in-car AR system using a video see-through\nhead-mounted display to engage with POIs via eye-gaze and pinch. Users could\nexplore passed and upcoming POIs using three visualization techniques: List,\nTimeline, and Minimap. We evaluated the system's feasibility in a field study\n(N=21). Our findings indicate general acceptance of the system, with the List\nvisualization being the preferred method for exploring POIs. Additionally, the\nstudy highlights limitations of current AR hardware, particularly the impact of\nvehicle movement on 3D interaction.",
        "link": "http://arxiv.org/abs/2502.08437v1",
        "published_date": "2025-02-12T14:22:45Z",
        "doi": "10.1145/3706598.3714323"
    },
    {
        "title": "Robot-Initiated Social Control of Sedentary Behavior: Comparing the\n  Impact of Relationship- and Target-Focused Strategies",
        "authors": [
            "Jiaxin Xu",
            "Sterre Anna Mariam van der Horst",
            "Chao Zhang",
            "Raymond H. Cuijpers",
            "Wijnand A. IJsselsteijn"
        ],
        "abstract": "To design social robots to effectively promote health behavior change, it is\nessential to understand how people respond to various health communication\nstrategies employed by these robots. This study examines the effectiveness of\ntwo types of social control strategies from a social robot,\nrelationship-focused strategies (emphasizing relational consequences) and\ntarget-focused strategies (emphasizing health consequences), in encouraging\npeople to reduce sedentary behavior. A two-session lab experiment was conducted\n(n = 135), where participants first played a game with a robot, followed by the\nrobot persuading them to stand up and move using one of the strategies. Half of\nthe participants joined a second session to have a repeated interaction with\nthe robot. Results showed that relationship-focused strategies motivated\nparticipants to stay active longer. Repeated sessions did not strengthen\nparticipants' relationship with the robot, but those who felt more attached to\nthe robot responded more actively to the target-focused strategies. These\nfindings offer valuable insights for designing persuasive strategies for social\nrobots in health communication contexts.",
        "link": "http://arxiv.org/abs/2502.08428v1",
        "published_date": "2025-02-12T14:13:38Z",
        "doi": null
    },
    {
        "title": "Word Synchronization Challenge: A Benchmark for Word Association\n  Responses for LLMs",
        "authors": [
            "Tanguy Cazalets",
            "Joni Dambre"
        ],
        "abstract": "This paper introduces the Word Synchronization Challenge, a novel benchmark\nto evaluate large language models (LLMs) in Human-Computer Interaction (HCI).\nThis benchmark uses a dynamic game-like framework to test LLMs ability to mimic\nhuman cognitive processes through word associations. By simulating complex\nhuman interactions, it assesses how LLMs interpret and align with human thought\npatterns during conversational exchanges, which are essential for effective\nsocial partnerships in HCI. Initial findings highlight the influence of model\nsophistication on performance, offering insights into the models capabilities\nto engage in meaningful social interactions and adapt behaviors in human-like\nways. This research advances the understanding of LLMs potential to replicate\nor diverge from human cognitive functions, paving the way for more nuanced and\nempathetic human-machine collaborations.",
        "link": "http://arxiv.org/abs/2502.08312v1",
        "published_date": "2025-02-12T11:30:28Z",
        "doi": null
    },
    {
        "title": "From Clicks to Conversations: Evaluating the Effectiveness of\n  Conversational Agents in Statistical Analysis",
        "authors": [
            "Qifu Wen",
            "Prishita Kochhar",
            "Sherif Zeyada",
            "Tahereh Javaheri",
            "Reza Rawassizadeh"
        ],
        "abstract": "The rapid proliferation of data science forced different groups of\nindividuals with different backgrounds to adapt to statistical analysis. We\nhypothesize that conversational agents are better suited for statistical\nanalysis than traditional graphical user interfaces (GUI). In this work, we\npropose a novel conversational agent, StatZ, for statistical analysis. We\nevaluate the efficacy of StatZ relative to established statistical\nsoftware:SPSS, SAS, Stata, and JMP in terms of accuracy, task completion time,\nuser experience, and user satisfaction. We combined the proposed analysis\nquestion from state-of-the-art language models with suggestions from\nstatistical analysis experts and tested with 51 participants from diverse\nbackgrounds. Our experimental design assessed each participant's ability to\nperform statistical analysis tasks using traditional statistical analysis tools\nwith GUI and our conversational agent. Results indicate that the proposed\nconversational agents significantly outperform GUI statistical software in all\nassessed metrics, including quantitative (task completion time, accuracy, and\nuser experience), and qualitative (user satisfaction) metrics. Our findings\nunderscore the potential of using conversational agents to enhance statistical\nanalysis processes, reducing cognitive load and learning curves and thereby\nproliferating data analysis capabilities, to individuals with limited knowledge\nof statistics.",
        "link": "http://arxiv.org/abs/2502.08114v1",
        "published_date": "2025-02-12T04:35:23Z",
        "doi": null
    }
]